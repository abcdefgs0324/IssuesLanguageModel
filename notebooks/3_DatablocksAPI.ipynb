{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from fastai.text import TextLMDataBunch as lmdb\n",
    "from fastai.text.transform import Tokenizer\n",
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "from random import shuffle"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Get list of pre-processed data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[PosixPath('data/mdparsed/processed_part0057.csv'),\n",
       " PosixPath('data/mdparsed/processed_part0043.csv'),\n",
       " PosixPath('data/mdparsed/processed_part0024.csv'),\n",
       " PosixPath('data/mdparsed/processed_part0054.csv'),\n",
       " PosixPath('data/mdparsed/processed_part0021.csv'),\n",
       " PosixPath('data/mdparsed/processed_part0093.csv'),\n",
       " PosixPath('data/mdparsed/processed_part0001.csv'),\n",
       " PosixPath('data/mdparsed/processed_part0002.csv'),\n",
       " PosixPath('data/mdparsed/processed_part0096.csv'),\n",
       " PosixPath('data/mdparsed/processed_part0032.csv')]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "p = Path('data/mdparsed')\n",
    "files = p.ls()\n",
    "shuffle(files)\n",
    "files[:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Read in Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# valid_df = pd.concat(pd.read_csv(f) for f in files[:10])\n",
    "# train_df = pd.concat(pd.read_csv(f) for f in files[10:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# valid_df.to_pickle('valid_df.pkl')\n",
    "# train_df.to_pickle('train_df.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "valid_df = pd.read_pickle('valid_df.pkl').dropna().drop_duplicates()\n",
    "train_df = pd.read_pickle('train_df.pkl').dropna().drop_duplicates()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f'rows in train_df:, {train_df.shape[0]:,}')\n",
    "print(f'rows in valid_df:, {valid_df.shape[0]:,}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Instantiate Your Own Tokenizer\n",
    "\n",
    "Because we already did lots of pre-procesing in `Test_Concurrency.ipynb`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pass_through(x):\n",
    "    return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# only thing is we are changing pre_rules to be pass through since we have already done all of the pre-rules.  \n",
    "# you don't want to accidentally apply pre-rules again otherwhise it will corrupt the data.\n",
    "tokenizer = Tokenizer(pre_rules=[pass_through], n_cpus=30)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Specify path for saving language model artifacts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !mkdir lang_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "path = Path('lang_model/')\n",
    "path.absolute()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Note you want your own tokenizer, without pre-rules\n",
    "data_lm = lmdb.from_df(path=path,\n",
    "                       train_df=train_df,\n",
    "                       valid_df=valid_df,\n",
    "                       text_cols='text',\n",
    "                       tokenizer=tokenizer,\n",
    "                       chunksize=6000000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_lm.save() # saves to self.path/data_save.pkl"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From the docs for [load_data](https://docs.fast.ai/basic_data.html#load_data):\n",
    "\n",
    "```\n",
    " Important: The arguments you passed when you created your first `DataBunch` aren't saved, so you should pass them here if you don't want the default.\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total 27G\r\n",
      "drwxr-xr-x 2 root root 6.0K May 14 03:59 .\r\n",
      "drwxr-xr-x 6 root root 6.0K May 14 03:59 ..\r\n",
      "-rw-r--r-- 1 root root  27G May 14 03:57 data_save.pkl\r\n"
     ]
    }
   ],
   "source": [
    "!ls -lah lang_model/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Notes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you don't have lots of data, you probably want to avoid multi-processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 24 ms, sys: 20.7 s, total: 20.7 s\n",
      "Wall time: 22.3 s\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[['xxmaj', 'hello', 'world', ',', 'i', 'am', 'here', '!', '!', 'weee', '.']]"
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%time\n",
    "tokenizer.process_all(['Hello world, I am here!! weee.'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 56 ms, sys: 4 ms, total: 60 ms\n",
      "Wall time: 60.9 ms\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[['xxmaj', 'hello', 'world', ',', 'i', 'am', 'here', '!', '!', 'weee', ',']]"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%time\n",
    "tokenizer._process_all_1(['Hello world, I am here!! weee,'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TextLMDataBunch;\n",
       "\n",
       "Train: LabelList (16385650 items)\n",
       "x: LMTextList\n",
       "xxbos xxxfldtitle xxunk throws exception when adding product via xxup api xxxfldbody xxmaj if xxmaj google xxmaj contents xxmaj experiments is enabled , adding a product using the xxxcdb xxup v1 / products / xxxcde endpoint causes xxunk xxxfilepath line 117 to throw xxxcdb invalidargumentexception xxxcde . xxmaj with contents experiments disabled the product add completes successfully . xxmaj example valid request json : \n",
       "  xxxcdb \" product \" : xxxjson \n",
       "  xxxcde,xxbos xxxfldtitle xxmaj grafana xxmaj kairosdb xxmaj top n rows xxxfldbody xxmaj when xxmaj grafana pulls the data it shows all the rows returned by the query , so can we please have a query option with something like xxunk ? xxmaj so we can only show limited number of rows on xxmaj grafana unlike now it just gets and shows everything . i have also requested a xxup ui option with xxmaj grafana guys where we can choose the number of rows we want to see under the graph , cheers,xxbos xxxfldtitle xxmaj graph request : mouseover highlight curve . xxxfldbody xxmaj it would be very nice , is mouseover of an item in graph legend ( to the left ) , or mouseover a graphed value ( with checkbox like xxup xxunk ) - would plot that value with double thickness . xxmaj that would make it easy to identify line for each value .,xxbos xxxfldtitle xxmaj graphite template to handle ip - address and hostname xxxfldbody xxmaj are there any plans to improve the graphite template logic ? xxmaj we are sending graphite data that contains xxup ip address and hostname ( xxunk , xxunk , etc ) , since those values have dot 's in them , it makes it difficult to identify tags for those graphite values . xxmaj one other issue we are having is identifying multiple tags in the middle of the graphite string to create a measurement .,xxbos xxxfldtitle xxmaj graphs : xxunk xxxfldbody xxmaj project : xxmaj graphs \n",
       "  xxmaj job : xxup uat \n",
       "  xxmaj env : xxup uat \n",
       "  xxmaj region : fxlabs / xxup us_west_1 \n",
       "  xxmaj result : fail \n",
       "  xxmaj status xxmaj code : 400 \n",
       "  xxmaj headers : xxunk x - xxup xss - xxmaj protection=[1 ; mode = block ] , xxmaj cache - xxmaj control=[no - cache , no - store , max - age=0 , must - revalidate ] , xxmaj pragma=[no - cache ] , xxmaj expires=[0 ] , x - xxmaj frame - options=[deny ] , xxunk xxmaj transfer - xxmaj encoding=[chunked ] , date=[tue , 02 xxmaj oct 2018 xxunk xxup gmt ] } \n",
       "  xxmaj endpoint : * xxup url * xxxlnkhb xxunk xxxlnkhe \n",
       "  xxmaj request : \n",
       "  xxmaj response : \n",
       "  xxxjson \n",
       "  xxmaj logs : \n",
       "  xxmaj assertion xxxatmention = = 403 ] resolved - to [ 400 = = 403 ] result [ xxmaj failed ] --- xxup fx xxmaj bot ---\n",
       "y: LMLabelList\n",
       ",,,,\n",
       "Path: lang_model;\n",
       "\n",
       "Valid: LabelList (1862119 items)\n",
       "x: LMTextList\n",
       "xxbos xxxfldtitle xxmaj gopi xxmaj episode 98 xxxfldbody xxmaj gopi xxmaj episode 98 xxxhtml * xxup url * xxxlnkhb ift.tt xxxlnkhe xxxhtml via xxmaj juragan xxmaj sinopsis * xxup url * xxxlnkhb ift.tt xxxlnkhe xxxhtml xxmaj december 17 , 2016 at xxup xxunk,xxbos xxxfldtitle xxmaj grabber with default xxunk model xxxfldbody xxxhm xxmaj overview \n",
       "  i want to use the default xxunk model ( loaded via dll ) to work with the grabber script . xxmaj the xxunk - demo uses xxunk controller block - shaped prefabs to add the grabber script , but i would like to actually use the default models . xxmaj how would i go about doing so ? \n",
       "  xxxhm xxmaj unity xxmaj editor xxmaj version \n",
       "  xxunk \n",
       "  xxxhm xxmaj mixed xxmaj reality xxmaj toolkit xxmaj release xxmaj version \n",
       "  xxunk xxmaj hot xxmaj fix,xxbos xxxfldtitle graphql integration xxxfldbody xxmaj hi ! xxmaj this project looks really promising to build a modern webapp 😊 i 've been hoping for something like this for a long time ! xxmaj however my first criterion is integrating with a graphql backend , which i even plan to be a hosted backend ( xxmaj graphcool xxxlnkhb * xxup url * xxxlnkhe ) . i suppose it 's not possible right now to use xxmaj xxunk in conjunction with xxmaj graphcool , but could it be considered in the design ? xxmaj the best would be to come up with a standard which can be plugged into any graphql backend ( hosted or own ) . xxmaj thanks so much and all the best,xxbos xxxfldtitle xxmaj grass toolbox filter input is n't labelled xxxfldbody xxxhr xxmaj author xxmaj name : xxunk - ( xxunk - ) xxmaj original xxmaj redmine xxmaj issue : xxunk , * xxup url * xxxlnkhb issues.qgis.org xxxlnkhe \n",
       "  xxxhr xxmaj if you open the xxup grass tools and choose the module list tab , there 's a text input field below the list which filters the list of modules . xxmaj however there 's no clue that it 's a filter box ! xxmaj we had some students confused by this on a course recently . \n",
       "  xxmaj needs a xxmaj filter : label on that line . xxmaj relevant xxup ui file is xxunk i guess .,xxbos xxxfldtitle xxmaj great program -- problems with kotlin xxxfldbody xxmaj quickly trying out your program - awesome . a few quick problems : decompiling a directory of kotlin ... a crash and some mis - ordered xxunk . crash : \n",
       "  xxunk xxxlnkhb github.com xxxlnkhe xxmaj xxunk : \n",
       "  xxup xxunk xxxlnkhb github.com xxxlnkhe xxunk xxxlnkhb github.com xxxlnkhe\n",
       "y: LMLabelList\n",
       ",,,,\n",
       "Path: lang_model;\n",
       "\n",
       "Test: None"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_lm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[tensor([[  88,   14,   28,  ...,   56,  125, 1370],\n",
       "         [   0,   33,   25,  ...,    5,   92,  242],\n",
       "         [  48,   14,   76,  ..., 1700,   16, 4579],\n",
       "         ...,\n",
       "         [ 112,  194,    9,  ..., 8712,   36,    9],\n",
       "         [  13,   68,   30,  ...,   14,   27,  229],\n",
       "         [   6, 2286, 3982,  ..., 1405,   65,   25]]),\n",
       " tensor([[   14,    28,     2,  ...,   125,  1370,    15],\n",
       "         [   33,    25,     6,  ...,    92,   242,    53],\n",
       "         [   14,    76,     5,  ...,    16,  4579,    25],\n",
       "         ...,\n",
       "         [  194,     9,   659,  ...,    36,     9,   287],\n",
       "         [   68,    30,    10,  ...,    27,   229, 12254],\n",
       "         [ 2286,  3982,    23,  ...,    65,    25,    72]])]"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "next(iter(data_lm.train_dl))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "tstbatch = _"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([   88,    14,    28,     2,    22,     5,  3332,     5, 55055,     5,\n",
       "          504,   678,  1535,    23,     5,    51,     5,  3332,  5974,     9,\n",
       "          110,    30,   457,    83,     9,  1535,   825,    75,     9,   462,\n",
       "           11,    89,    57,    64,   146,    56,    15,   462,   244,    40,\n",
       "          209,    87,     0,    43,     5,    89,    64,    57,   105,   251,\n",
       "         1993,   260,    26,  1535,    36,     5,  3332,  5092,   179,    30,\n",
       "          120,   668,    24,   457,   601,    10,    19,    56,   125,  1370])"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#first x\n",
    "tstbatch[0][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([    0,    33,    25,     6,   187,    17,     5,  1424,    13,  9788,\n",
       "          207,     5,  3234,   518, 10835,   124,    61,  3076,    94,    25,\n",
       "         3126,    37, 10881,    31,  2831,   262,  2003,    17,     2,    22,\n",
       "          450,   159,   462,   340,    13,   901,    25,    34,  1432,   427,\n",
       "           13,   133,   371,    34,    17,    23,    76,     5,   165,     5,\n",
       "          308,    14,     5,   146,  1272,   216,   827,   238,  1245,   115,\n",
       "           77,    10,     5,   200,    94,    14,    71,     5,    92,   242])"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#second batch of x\n",
    "tstbatch[0][1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([   14,    28,     2,    22,     5,  3332,     5, 55055,     5,   504,\n",
       "          678,  1535,    23,     5,    51,     5,  3332,  5974,     9,   110,\n",
       "           30,   457,    83,     9,  1535,   825,    75,     9,   462,    11,\n",
       "           89,    57,    64,   146,    56,    15,   462,   244,    40,   209,\n",
       "           87,     0,    43,     5,    89,    64,    57,   105,   251,  1993,\n",
       "          260,    26,  1535,    36,     5,  3332,  5092,   179,    30,   120,\n",
       "          668,    24,   457,   601,    10,    19,    56,   125,  1370,    15])"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# first target\n",
    "tstbatch[1][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(Text xxbos xxxfldtitle xxunk throws exception when adding product via xxup api xxxfldbody xxmaj if xxmaj google xxmaj contents xxmaj experiments is enabled , adding a product using the xxxcdb xxup v1 / products / xxxcde endpoint causes xxunk xxxfilepath line 117 to throw xxxcdb invalidargumentexception xxxcde . xxmaj with contents experiments disabled the product add completes successfully . xxmaj example valid request json : \n",
       "   xxxcdb \" product \" : xxxjson \n",
       "   xxxcde, EmptyLabel )"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_lm.train_ds[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
